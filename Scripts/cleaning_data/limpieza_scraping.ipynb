{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Variables\n",
    "'''\n",
    "# This code make the next things:\n",
    "    1. Unify all the docs created using the bot to get the URL's linkedin for the people in two different files; People with Linkedin Url and people without Linkedin URL\n",
    "    2. Clean the data from the both mentioned files above\n",
    "    3. \n",
    "'''\n",
    "path_data = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/user/Desktop/MateoCodes/WebScrapingLinkedin'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio de agrupación de archivos... \n",
      "\n",
      "Agrupacion de archivos finalizado\n"
     ]
    }
   ],
   "source": [
    "# 1. Unify all the docs created using the bot to get the URL's linkedin for the people in two different files; People with Linkedin Url and people without Linkedin URL\n",
    "\n",
    "\n",
    "# Get the list of all the files that have the word \"egresados\" that are the files with the URL's linkedin for the people\n",
    "\n",
    "path_graduates = os.path.abspath(os.path.join(path_data,\"Data\",\"Links_Linkedin\",\"Bot_Google_Buscador_Perfiles\"))\n",
    "\n",
    "files_graduates = [f for f in os.listdir(path_graduates) if 'egresados' in f]\n",
    "\n",
    "# Open the files and Aggregate the information\n",
    "\n",
    "# New DataFrame with all the information about the links of the people\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "print(\"Inicio de agrupación de archivos... \\n\")\n",
    "\n",
    "for file in files_graduates:\n",
    "    path_file_graduated_temp = os.path.join(path_graduates,file)\n",
    "    \n",
    "    # Read the excel and get the information\n",
    "\n",
    "    df_temp_file = pd.read_excel(path_file_graduated_temp)\n",
    "\n",
    "    # Concat the last excel with the big DataFrame\n",
    "    all_data = pd.concat([all_data,df_temp_file])\n",
    "\n",
    "print(\"Agrupacion de archivos finalizado\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of two DataFrames with the people who has linkedin url and who didn't have\n",
    "\n",
    "linkedin_people = all_data[all_data['URL'].str.contains('linkedin',na = False)]\n",
    "non_linkedin_people = all_data[~all_data['URL'].str.contains('linkedin', na=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning for linkedin people & not linkedin people\n",
    "df_clean_linkedin_people = linkedin_people.drop_duplicates().drop_duplicates(subset=['Nombre'], keep='first')\n",
    "df_clean_not_linkedin_people = non_linkedin_people.drop_duplicates().drop_duplicates(subset=['Nombre'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the cleaned data \n",
    "path_export_data = os.path.abspath(os.path.join(path_data,\"Data\",\"Links_Linkedin\",\"Datos_Finales_Perfiles_Linkedin\"))\n",
    "\n",
    "df_clean_linkedin_people.to_excel(path_export_data + \"/CleanDataLinkedinProfiles.xlsx\")\n",
    "df_clean_not_linkedin_people.to_excel(path_export_data + \"/CleanDataNotLinkedinProfiles.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Important code for get all the information in the same format\n",
    "    1. Create a regular expression for all the links \n",
    "    2. Get the links for the people who the first bot don't get a specific url because it finds a link for search the people\n",
    "    3. Export the links people search (mentionead above) for use in the second bot and find the information\n",
    "'''\n",
    "\n",
    "pattern = r\"linkedin\\.com\\/(in\\/[a-zA-Z0-9\\-_%]+)\"\n",
    "cont = 0\n",
    "\n",
    "search_people_link = []\n",
    "for row,columns in df_clean_linkedin_people.iterrows():\n",
    "    \n",
    "    data = columns['URL']\n",
    "    match = re.search(pattern, data)\n",
    "    try:\n",
    "        a = (f\"https://www.linkedin.com/{match.group(1)}\")\n",
    "    except:\n",
    "        if 'pub' in data:\n",
    "            search_people_link.append(columns['Nombre'])\n",
    "            cont += 1\n",
    "\n",
    "print(cont)\n",
    "\n",
    "with open(os.path.abspath(os.path.join(path_data,\"Data\",\"Links_Linkedin\",\"BD_egresados\",\"Data_Bot/search_people_linkedin.txt\")),\"w+\") as f:\n",
    "    for people in search_people_link:\n",
    "        f.write(people)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important !!!\n",
    "\n",
    "# For the next steps you must to run the second bot located in the folder Scripts/get_data/Second_bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar checkout y obtener enlace nuevo\n",
    "df_checkout = pd.read_excel('/home/user/Desktop/MateoCodes/WebScrapingLinkedin/documentacion/NEW_DATA/clean_people_get_link.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_nuevo_scrap = {}\n",
    "dict_sin_checkpoint = {}\n",
    "\n",
    "for index, row in df_checkout.iterrows():\n",
    "    url = row['URL'].strip()\n",
    "    \n",
    "    if 'checkpoint' in url:\n",
    "        dict_nuevo_scrap[row['Nombre']] = row['URL']\n",
    "    else :\n",
    "        dict_sin_checkpoint[row['Nombre']] = row['URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nuevo_scrap = pd.DataFrame.from_dict([dict_nuevo_scrap]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nuevo_scrap.to_excel(\"/home/user/Desktop/MateoCodes/WebScrapingLinkedin/documentacion/NEW_DATA/people_checkpoint.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4064"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get profiles without checkpoint\n",
    "list(dict_sin_checkpoint.values())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
